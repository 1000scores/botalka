{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kek.ipynb\n"
     ]
    }
   ],
   "source": [
    "print('kek.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0\n",
      "Is MPS (Metal Performance Shader) built? True\n",
      "Is MPS available? True\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check PyTorch has access to MPS (Metal Performance Shader, Apple's GPU architecture)\n",
    "print(f\"Is MPS (Metal Performance Shader) built? {torch.backends.mps.is_built()}\")\n",
    "print(f\"Is MPS available? {torch.backends.mps.is_available()}\")\n",
    "\n",
    "# Set the device      \n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import timeit\n",
    "import functools\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "START=100\n",
    "END=200\n",
    "GAP = 20\n",
    "\n",
    "\n",
    "def helper_func(shapes,pytorch_func,display_name=\"\"):\n",
    "    ts = [torch.randn(x, device=\"cpu\",requires_grad=True) for x in shapes]\n",
    "    out = pytorch_func(*ts)\n",
    "    out.mean().backward()\n",
    "    torch_fp = timeit.Timer(functools.partial(pytorch_func, *ts)).timeit(5) * 1000/5\n",
    "    torch_fbp = timeit.Timer(functools.partial(lambda f,x: f(*x).mean().backward(), pytorch_func, ts)).timeit(5) * 1000/5\n",
    "    print(\"testing(%s)  %40r    torch fp: %.2fms  bp: %.2fms\" % (display_name,shapes, torch_fp, torch_fbp-torch_fp))\n",
    "    return [display_name,shapes, str(torch_fp)[:6], str(torch_fbp-torch_fp)[:6]]\n",
    "\n",
    "\"\"\"#*******Display results in a table********\n",
    "def display_results(results):\n",
    "    x = PrettyTable()\n",
    "    x.field_names = [\"Operation\",\"Shapes\",\"Forward Prop\",\"Backprop\"]\n",
    "    for result in results:\n",
    "        x.add_row(result)\n",
    "    print(x)\"\"\"\n",
    "\n",
    "#**********TEST FUNCTIONS**********\n",
    "def stress_test(func_name,stress_level = 10):\n",
    "    results = []\n",
    "    for ordermag in range(stress_level):\n",
    "        offset = np.random.randint(10**ordermag,10**(ordermag+1))\n",
    "        results.append(func_name(offset))\n",
    "    display_results(results)\n",
    "\n",
    "\n",
    "def test_add(dim):\n",
    "    return helper_func([(dim,dim), (dim,dim)], lambda x,y: x+y,\"Add\")\n",
    "def test_sub(dim):\n",
    "    return helper_func([(dim,dim), (dim,dim)], lambda x,y: x-y,\"Sub\")\n",
    "def test_mul(dim):\n",
    "    return helper_func([(dim,dim), (dim,dim)], lambda x,y: x*y,\"Multiply\")\n",
    "def test_div(dim):\n",
    "    return helper_func([(dim,dim), (dim,dim)], lambda x,y: x/y,\"Divide\")\n",
    "def test_pow(dim):\n",
    "    return helper_func([(dim,dim), (dim,dim)], lambda x,y: x**y,\"Power\")\n",
    "def test_sqrt(dim):\n",
    "    return helper_func([(dim,dim)], lambda x: x.sqrt(),\"Sqrt\")\n",
    "def test_relu(dim):\n",
    "    return helper_func([(dim,dim)], lambda x: x.relu(),\"ReLU\")\n",
    "def test_leakyrelu(dim):\n",
    "    return helper_func([(dim,dim)], lambda x: torch.nn.functional.leaky_relu(x,0.01),\"LReLU\")\n",
    "def test_abs(dim):\n",
    "    return helper_func([(dim,dim)], lambda x: torch.abs(x),\"Abs\")\n",
    "def test_log(dim):\n",
    "    return helper_func([(dim,dim)], lambda x: torch.log(x),\"Log\")\n",
    "def test_exp(dim):\n",
    "    return helper_func([(dim,dim)], lambda x: torch.exp(x),\"Exp\")\n",
    "def test_sign(dim):\n",
    "    return helper_func([(dim,dim)], lambda x: torch.sign(x),\"Sign\")\n",
    "def test_sigmoid(dim):\n",
    "    return helper_func([(dim,dim)], lambda x: x.sigmoid(),\"Sigmoid\")\n",
    "def test_softplus(dim):\n",
    "    return helper_func([(dim,dim)], lambda x: torch.nn.functional.softplus(x),\"Softplus\")\n",
    "def test_relu6(dim):\n",
    "    helper_func([(dim,dim)], lambda x: torch.nn.functional.relu6(x),\"ReLU6\")\n",
    "def test_hardswish(dim):\n",
    "    helper_func([(dim,dim)], lambda x: torch.nn.functional.hardswish(x),\"Hardswish\")\n",
    "#mish(x) =  x*tanh(softplus(x))\n",
    "def test_mish(dim):\n",
    "    def _calc_mish(x):\n",
    "        return x*torch.tanh(torch.nn.functional.softplus(x))\n",
    "    return helper_func([(dim,dim)],_calc_mish,\"mish\")\n",
    "def test_dot(dim):\n",
    "    dim1 = np.random.randint(dim//2,dim)\n",
    "    return helper_func([(dim,dim1),(dim1,dim)],lambda x,y: x.matmul(y),\"matmul\")\n",
    "\n",
    "#multiple for loops, gonna have to break it up?\n",
    "def test_dot2D(dim):\n",
    "    dim1 = np.random.randint(dim//2,dim)\n",
    "    return helper_func([(dim,dim1),(dim1,dim)],lambda x,y: x @ y,\"dot2D\")\n",
    "\n",
    "def test_dot3D(dim):\n",
    "    dim1 = np.random.randint(dim//2,dim)\n",
    "    dim3 = np.random.randint(dim//2,dim)\n",
    "    return helper_func([(dim3,dim,dim1),(dim3,dim1,dim)],lambda x,y: x @ y,\"dot3D\")\n",
    "\n",
    "def test_dot4D(dim):\n",
    "    results = []\n",
    "    gdim = 100\n",
    "    dim3 = np.random.randint(gdim//2,gdim)\n",
    "    dim4 = np.random.randint(gdim//16,gdim//8)\n",
    "    return helper_func([(dim4,dim3,dim,dim1),(dim4,dim3,dim1,dim)],lambda x,y: x @ y,\"dot4D\")\n",
    "\n",
    "\n",
    "\"\"\"def test_sum(dim):\n",
    "    # modify -- does not work properly\n",
    "    results = []\n",
    "    dim1 = np.random.randint(END//2,END)\n",
    "    dim2 = np.random.randint(END//2,END)\n",
    "    dim3 = np.random.randint(END//2,END)\n",
    "    dim4 = np.random.randint(END//2,END)\n",
    "    for dim in range(dim):\n",
    "        results.append(helper_func([(dim2,dim1)],lambda x: x.sum()),\"sum2D\")\n",
    "    for dim in range(dim):\n",
    "        results.append(helper_func([(dim4,dim3,dim2,dim1)],lambda x: x.sum(axis=(1,2)),\"sum4D,(1,2)\"))\n",
    "    for dim in range(dim):\n",
    "        results.append(helper_func([(dim4,dim3,dim2,dim1)],lambda x: x.sum(axis=(1)),\"sum4D,(1)\"))\n",
    "    display_results(result)\"\"\"\n",
    "def test_mean_axis(dim):\n",
    "    return helper_func([(dim,2*dim,3*dim,4*dim)],lambda x: x.mean(axis=(1,2)),\"Mean\")\n",
    "def test_logsoftmax(dim):\n",
    "    return helper_func([(dim,dim)],lambda x: torch.nn.LogSoftmax(dim=1)(x),\"LogSoftmax\")\n",
    "def test_tanh(dim):\n",
    "    return helper_func([(dim,dim)],lambda x: x.tanh()),\"Tanh\"\n",
    "def test_topo_sort(dim):\n",
    "    return helper_func([(dim,dim)],lambda x: (x+x)*x),\"Topo Sort??\"\n",
    "def test_scalar_mul(dim):\n",
    "    scalar_val = np.random.randint()\n",
    "    return helper_func([(dim,dim)],lambda x: x*scalar_val,\"Scalar Mult\")\n",
    "def test_scalar_rmul(dim):\n",
    "    scalar_val = np.random.randint()\n",
    "    return helper_func([(dim,dim)],lambda x: scalar_val*x,\"Reverse Scalar Mult\")\n",
    "def test_scalar_sub(dim):\n",
    "    scalar_val = np.random.randint()\n",
    "    return helper_func([(dim,dim)],lambda x: x-scalar_val,\"Scalar Subtraction\")\n",
    "def test_scalar_rsub(dim):\n",
    "    scalar_val = np.random.randint()\n",
    "    return helper_func([(dim,dim)],lambda x: scalar_val-x,\"Reverse Scalar Mult\")\n",
    "def test_slice(dim):\n",
    "    random_slice = np.random.randint(START,END)\n",
    "def test_pad2d(dim):\n",
    "    return helper_func([(dim,dim,dim,dim)],lambda x: torch.nn.functional.pad(x,(1,2,3,4)),\"Pad2D\")\n",
    "def test_transpose(dim):\n",
    "    return helper_func([(dim,dim,dim,dim)],lambda x: x.movedim((3,2,1,0),(0,1,2,3)),\"Transpose\")\n",
    "def test_reshape(dim):\n",
    "    return helper_func([(dim//4,dim//2,dim,dim)],lambda x: torch.reshape(x,(-1,dim//2,dim,dim)),\"Reshape\")\n",
    "\n",
    "# gonna have to come back to these, need to take another look at the shapes\n",
    "def test_conv2d(bs,cin,groups,H,W):\n",
    "    return helper_func([bs,cin,11,])\n",
    "def test_strided_conv2d():\n",
    "    pass\n",
    "def test_maxpool_2d():\n",
    "    pass\n",
    "def test_avgpool2d():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing(Reshape)                            [(0, 1, 2, 2)]    torch fp: 0.00ms  bp: 0.06ms\n",
      "testing(Reshape)                         [(6, 12, 24, 24)]    torch fp: 0.00ms  bp: 0.21ms\n"
     ]
    }
   ],
   "source": [
    "stress_test(test_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
